{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99734e8-4f72-417a-a09a-fe575008db39",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a0adc-91f9-492a-9653-ceb136ab459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library ---\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "#BEFORE importing transformers\n",
    "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
    "\n",
    "# --- Third-party: numerics / plotting ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "\n",
    "\n",
    "\n",
    "# --- Third-party: PyTorch stack ---\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import accelerate\n",
    "from torch.optim import AdamW\n",
    "# --- Third-party: scikit-learn ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- Third-party: Transformers ---\n",
    "import transformers  \n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    AutoModel,\n",
    "    \n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969fff6-c08b-45ee-aa03-dfe8d45ad738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data sets\n",
    "df = pd.read_csv(\"hf://datasets/StephanAkkerman/financial-tweets/financial_tweets.csv\")\n",
    "stock_df = pd.read_csv('stock_tweets.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86598eed-af97-49d8-8923-61e05529b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns\n",
    "sentiment_df = df[[\"description\", \"sentiment\"]].copy()\n",
    "\n",
    "# Filter only valid sentiment labels\n",
    "sentiment_df = sentiment_df[sentiment_df[\"sentiment\"].isin([\"Bullish\", \"Neutral\", \"Bearish\"])]\n",
    "\n",
    "# Map text labels to numeric values\n",
    "label_map = {\"Bearish\": 0, \"Neutral\": 1, \"Bullish\": 2}\n",
    "sentiment_df[\"sentiment\"] = sentiment_df[\"sentiment\"].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce49888-89b2-4a74-a6c5-a6f9a0996af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head using the familiar Pandas method\n",
    "print(sentiment_df.columns)\n",
    "print(stock_df.columns)\n",
    "\n",
    "print(f\"\\nTotal elements in sentiment_df (via shape): {sentiment_df.shape[0]}\")\n",
    "print(f\"Total elements in stock_df (via shape): {stock_df.shape[0]}\")\n",
    "# print(\"Sentiment Dataset Head:\")\n",
    "# print(sentiment_df.head())\n",
    "\n",
    "# print(\"\\nStock Dataset Info:\")\n",
    "# sentiment_df.info()\n",
    "\n",
    "# print(\"Sentiment Dataset Head:\")\n",
    "# print(sentiment_df.head())\n",
    "\n",
    "# print(\"\\nStock Dataset Info:\")\n",
    "# sentiment_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae0e04-b814-4821-bfb2-318a3b7519b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, label_column, title_prefix, color='skyblue'):\n",
    "    \"\"\"\n",
    "    Plots and prints the count and ratio distribution for a categorical column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to analyze.\n",
    "    label_column : str\n",
    "        The name of the column containing categorical labels (e.g., company, sentiment).\n",
    "    title_prefix : str\n",
    "        Prefix for the plot title (e.g., 'Company', 'Sentiment').\n",
    "    color : str, optional\n",
    "        Color for the bar plot (default is 'skyblue').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (counts, ratios) as pandas Series objects.\n",
    "    \"\"\"\n",
    "    # Count occurrences\n",
    "    counts = df[label_column].value_counts()\n",
    "    ratios = counts / counts.sum()\n",
    "\n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts.plot(kind='bar', color=color)\n",
    "    plt.title(f'{title_prefix} Distribution')\n",
    "    plt.xlabel(title_prefix)\n",
    "    plt.ylabel('Number of Records')\n",
    "    plt.xticks(rotation=45 if title_prefix.lower() == 'company' else 0, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display results\n",
    "    print(f\"=== {title_prefix} Ratios ===\")\n",
    "    print(ratios)\n",
    "    print(f\"\\n=== {title_prefix} Counts ===\")\n",
    "    print(counts)\n",
    "    \n",
    "    return counts, ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94746c7-28fe-47cb-87b7-f2e2e70ecdcc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Stock DataFrame\n",
    "stock_counts, stock_ratios = plot_distribution(\n",
    "    df=stock_df, \n",
    "    label_column='Stock Name', \n",
    "    title_prefix='Company',\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "# For Sentiment DataFrame\n",
    "sentiment_counts, sentiment_ratios = plot_distribution(\n",
    "    df=sentiment_df, \n",
    "    label_column='sentiment', \n",
    "    title_prefix='Sentiment',\n",
    "    color='lightcoral'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97f916-e259-481c-9906-c835eae86557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_subset(df_name: pd.DataFrame, df_column_name: str, num: int, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a roughly balanced subset of size `num` from the given DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_name : pd.DataFrame\n",
    "        The input dataset.\n",
    "    df_column_name : str\n",
    "        The name of the column containing class labels.\n",
    "    num : int\n",
    "        The total number of samples desired.\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility (default=42).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A subset of the original DataFrame with roughly balanced class distribution.\n",
    "    \"\"\"\n",
    "    # Unique classes and samples per class\n",
    "    classes = df_name[df_column_name].unique()\n",
    "    samples_per_class = num // len(classes)\n",
    "    \n",
    "    balanced_subset = []\n",
    "    \n",
    "    for c in classes:\n",
    "        class_subset = df_name[df_name[df_column_name] == c]\n",
    "        \n",
    "        # Sample from each class\n",
    "        subset = resample(class_subset,\n",
    "                          replace=False,\n",
    "                          n_samples=min(len(class_subset), samples_per_class),\n",
    "                          random_state=random_state)\n",
    "        balanced_subset.append(subset)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat(balanced_subset)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e90bc-5733-4ad3-a9b0-7f63fa812826",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_balanced = get_balanced_subset(df_name=stock_df, df_column_name='Stock Name', num=sentiment_df.shape[0])\n",
    "sentiment_classes = sentiment_df[\"sentiment\"].nunique()  # likely 3\n",
    "num2 = stock_df_balanced.shape[0] - (stock_df_balanced.shape[0] % sentiment_classes)\n",
    "# Trim stock to num2 as well so they match\n",
    "stock_df_balanced = stock_df_balanced.sample(n=num2, random_state=42).reset_index(drop=True)\n",
    "sentiment_df_balanced = get_balanced_subset(sentiment_df, 'sentiment', num=num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c614b00-ffa9-44e0-9c7f-c247997edab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Stock DataFrame\n",
    "stock_counts, stock_ratios = plot_distribution(\n",
    "    df=stock_df_balanced, \n",
    "    label_column='Stock Name', \n",
    "    title_prefix='Company',\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "# For Sentiment DataFrame\n",
    "sentiment_counts, sentiment_ratios = plot_distribution(\n",
    "    df=sentiment_df_balanced, \n",
    "    label_column='sentiment', \n",
    "    title_prefix='Sentiment',\n",
    "    color='lightcoral'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa01f4-4831-4b1a-9449-c87d909dc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be91be3-56e4-4924-8935-fd8c497a4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TweetNormalizer import normalizeTweet\n",
    "\n",
    "# =========================================\n",
    "# STOCK_DF PROCESSING\n",
    "# =========================================\n",
    "\n",
    "# 1. Create a unique pair identifier combining Stock Name and Company Name\n",
    "stock_df_processed = stock_df_balanced.copy()\n",
    "stock_df_processed[\"pair\"] = stock_df_processed[\"Stock Name\"] + \"___\" + stock_df_processed[\"Company Name\"]\n",
    "\n",
    "# 2) Build mapping: frequent pairs keep their own ID, others â†’ OTHERS\n",
    "MIN_COUNT = 200  #more than 200\n",
    "pair_counts = stock_df_processed[\"pair\"].value_counts()\n",
    "frequent = sorted(pair_counts.index[pair_counts > MIN_COUNT].tolist())\n",
    "\n",
    "pair_to_id = {p: i for i, p in enumerate(frequent)}\n",
    "OTHERS_ID = len(pair_to_id)\n",
    "\n",
    "stock_df_processed[\"label\"] = (\n",
    "    stock_df_processed[\"pair\"]\n",
    "    .where(stock_df_processed[\"pair\"].isin(frequent), \"OTHERS\")\n",
    "    .map({**pair_to_id, \"OTHERS\": OTHERS_ID})\n",
    ")\n",
    "\n",
    "# 3) Clean columns and text\n",
    "stock_df_processed.drop(columns=[\"pair\", \"Stock Name\", \"Date\", \"Company Name\"], inplace=True, errors=\"ignore\")\n",
    "stock_df_processed.rename(columns={\"Tweet\": \"text\"}, inplace=True)\n",
    "# 4. Normalize text\n",
    "stock_df_processed[\"text\"] = stock_df_processed[\"text\"].astype(str).apply(normalizeTweet)\n",
    "\n",
    "label2id = {**pair_to_id, \"OTHERS\": OTHERS_ID}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "\n",
    "# 7. Inspect\n",
    "print(\"Stock DataFrame Head after processing:\")\n",
    "print(stock_df_processed.head())\n",
    "print(\"\\nFinal Columns in stock_df_processed:\")\n",
    "print(stock_df_processed.columns)\n",
    "\n",
    "# =========================================\n",
    "# SENTIMENT_DF PROCESSING\n",
    "# =========================================\n",
    "\n",
    "sentiment_df_processed = sentiment_df_balanced.copy()\n",
    "\n",
    "# 1. Rename columns to match stock_df\n",
    "sentiment_df_processed.rename(columns={\"description\": \"text\", \"sentiment\": \"label\"}, inplace=True)\n",
    "sentiment_df_processed[\"text\"] = sentiment_df_processed[\"text\"].astype(str)\n",
    "\n",
    "# 2. Normalize text\n",
    "sentiment_df_processed[\"text\"] = sentiment_df_processed[\"text\"].apply(normalizeTweet)\n",
    "\n",
    "# 3. Inspect\n",
    "print(\"\\nSentiment DataFrame Head after processing:\")\n",
    "print(sentiment_df_processed.head())\n",
    "print(\"\\nFinal Columns in sentiment_df_processed:\")\n",
    "print(sentiment_df_processed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce83286-f10e-41e4-9c7a-88359aa5ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_counts, stock_ratios = plot_distribution(\n",
    "    df=stock_df_processed, \n",
    "    label_column='label', \n",
    "    title_prefix='Company',\n",
    "    color='skyblue'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db435ac8-716d-4726-9c3b-d3397a487c69",
   "metadata": {},
   "source": [
    "# Sentiment analysis (finBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2b423-98d0-43a7-bd30-fc9e3cb7de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and tokenize\n",
    "# SPlit the pre processed data into taining and validation. \n",
    "train, val = train_test_split(\n",
    "    sentiment_df_processed,\n",
    "    test_size=0.2,\n",
    "    stratify=sentiment_df_processed[\"label\"], # keeps the label distrubation for both sets\n",
    "    random_state=42, # makes it repeatble\n",
    ")\n",
    "\n",
    "# Pull tokenizer for finBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Tokenize train and val texts\n",
    "train_encodings = tokenizer(\n",
    "    train[\"text\"].tolist(), # input format\n",
    "    truncation=True, # cut sequences longer than max_length\n",
    "    padding=True, # padd shorter sequences than max_length\n",
    "    max_length=96,    \n",
    "    return_tensors=\"pt\"  # output format\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    val[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=96,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222507dd-dd21-4a28-af0d-854f69e623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create wrapper and datasets\n",
    "# Pytorch Dataset wrapper\n",
    "class FinBERTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Create two datasets\n",
    "train_dataset = FinBERTDataset(train_encodings, train[\"label\"].astype(int).tolist())\n",
    "val_dataset   = FinBERTDataset(val_encodings,   val[\"label\"].astype(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fecefc-8989-41a4-9385-42a0dcf91f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "# label mapping\n",
    "model.config.id2label = {0: \"Bearish\", 1: \"Neutral\", 2: \"Bullish\"}  # match your dataset meaning\n",
    "model.config.label2id = {\"Bearish\": 0, \"Neutral\": 1, \"Bullish\": 2}\n",
    "\n",
    "# unfreeze all layers(Full fine tuning) \n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# unfreeze classification layers(Fine tuning)\n",
    "#for param in model.bert.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "# Use mps on my mac /change when using VM)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76279aee-e882-4612-af4e-d07f7f7802ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy and F1 scores from model evaluation outputs.\n",
    "\n",
    "    Accepts either a (logits, labels) tuple or a transformers.EvalPrediction.\n",
    "\n",
    "    Args:\n",
    "        eval_pred: Tuple[np.ndarray, np.ndarray] or transformers.EvalPrediction\n",
    "            - logits: array of shape (num_examples, num_classes)\n",
    "            - labels: array of shape (num_examples,)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: {\n",
    "            \"accuracy\": overall accuracy,\n",
    "            \"f1_macro\": macro-averaged F1 across classes,\n",
    "            \"f1_weighted\": class-frequency-weighted F1\n",
    "        }\n",
    "    \"\"\"\n",
    "    if isinstance(eval_pred, tuple):\n",
    "        logits, labels = eval_pred\n",
    "    else:\n",
    "        logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb16e3e-b308-4248-849f-a53ba92ea74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./finbert_trained_tweets\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5, # try with 1e-5 and 3e-5\n",
    "    per_device_train_batch_size=16, # Try with 32\n",
    "    per_device_eval_batch_size=16, # Try with 32\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1, #initial ramp up phase\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1, \n",
    "    load_best_model_at_end=True, # save the model which perform best\n",
    "    metric_for_best_model=\"f1_macro\", # aiming for high f1 macro\n",
    "    greater_is_better=True, \n",
    "    report_to=[],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],#if f1 macro dosent improve for 2 epochs -> stop\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecafe38-f0fc-48f0-8fd4-ac2b8f6c273c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "trainer.train()\n",
    "# evalutet the best model and print the results\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# save best model\n",
    "trainer.save_model(\"./finbert_trained_tweets/best\")\n",
    "tokenizer.save_pretrained(\"./finbert_trained_tweets/best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b26ce2-ef55-46a3-9906-6256a2d05afe",
   "metadata": {},
   "source": [
    "# Company Classification (BERTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae1983-7a96-430b-9dac-4973f3a682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the pre processed data into taining and validation. \n",
    "train, val= train_test_split(\n",
    "    stock_df_processed,\n",
    "    test_size=0.2,\n",
    "    stratify=stock_df_processed[\"label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "# Pull tokenizer for BERTweet\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vinai/bertweet-base\",\n",
    "    use_fast=False, # Recommended to use slow tokenizer\n",
    "    normalization=True #tweet normalization (just to be safe)\n",
    ")\n",
    "\n",
    "# Tokenize train and val texts\n",
    "train_enc = tokenizer(\n",
    "    train[\"text\"].tolist(), # input format\n",
    "    truncation=True, # cut sequences longer than max_length\n",
    "    padding=True, # padd shorter sequences than max_length\n",
    "    max_length=96,\n",
    "    return_tensors=\"pt\" # output format\n",
    ")\n",
    "val_enc = tokenizer(\n",
    "    val[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=96,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe4544-9cb8-4500-aa2e-cc76c1e15726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create wrapper and datasets\n",
    "# Pytorch Dataset wrapper\n",
    "class TweetCompanyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create two datasets\n",
    "train_dataset = TweetCompanyDataset(train_enc, train[\"label\"].tolist())\n",
    "val_dataset   = TweetCompanyDataset(val_enc,   val[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabfc56-9285-4925-be8b-5952a4410842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label maps from dataset\n",
    "label2id = pair_to_id                       # str -> int\n",
    "id2label = {v: k for k, v in label2id.items()}  # int -> str\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# Load BERTweet with a classification head of size 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-base\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# unfreeze all layers(Full fine tuning) \n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# unfreeze classification layers(Fine tuning)\n",
    "#for param in model.bert.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Use mps on my mac /change when using VM)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912d0c9-9cf7-40a7-946b-98e09579421d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- TrainingArguments: select BEST CHECKPOINT BY ACCURACY ---\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./berttweet-company\",\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06, #initial ramp up phase\n",
    "    report_to=[],\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\", # aiming for high f1 macro\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    label_smoothing_factor=0.1,\n",
    "    gradient_accumulation_steps=2 \n",
    ")\n",
    "\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,           \n",
    "    data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a912e-6127-4205-a4c4-98fee5cb2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "trainer.train()\n",
    "\n",
    "# evalutet the best model and print the results\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save\n",
    "trainer.save_model(\"./berttweet-company/best\")\n",
    "tokenizer.save_pretrained(\"./berttweet-company/best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41414b-3df9-4905-af0a-a25eff85c813",
   "metadata": {},
   "source": [
    "# Function to pipeline both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c32c1-7b29-4e4c-9e86-fe6c3ca884a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "company_clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"./berttweet-company/best\",\n",
    "    tokenizer=\"./berttweet-company/best\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "sentiment_clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"./finbert_trained_tweets/best\",\n",
    "    tokenizer=\"./finbert_trained_tweets/best\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "def analyze_tweet_pretty(tweet: str):\n",
    "    text = tweet.strip()\n",
    "\n",
    "    # Company\n",
    "    comp_scores = company_clf([text])[0]  # list of\n",
    "    comp_top = max(comp_scores, key=lambda x: x[\"score\"])\n",
    "    # tidy up\n",
    "    comp_label = re.sub(r\"_+\", \"_\", comp_top[\"label\"]).strip(\"_\")\n",
    "    comp_score = round(float(comp_top[\"score\"]), 2)\n",
    "\n",
    "    # Sentiment (binary)\n",
    "    sent_scores = sentiment_clf([text])[0]\n",
    "    sent_top = max(sent_scores, key=lambda x: x[\"score\"])\n",
    "    raw_lbl = sent_top[\"label\"].lower()\n",
    "    if \"pos\" in raw_lbl:\n",
    "        sent_label = \"Bullish\"\n",
    "    elif \"neg\" in raw_lbl:\n",
    "        sent_label = \"Bearish\"\n",
    "    else:\n",
    "        # keep model label as is\n",
    "        sent_label = sent_top[\"label\"]\n",
    "    sent_score = round(float(sent_top[\"score\"]), 2)\n",
    "\n",
    "\n",
    "    return {comp_label: {\"score\": comp_score}}, {sent_label: {\"score\": sent_score}}\n",
    "\n",
    "# Example:\n",
    "analyze_tweet_pretty(\"Am considering taking Tesla private at $420. Funding secured.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
